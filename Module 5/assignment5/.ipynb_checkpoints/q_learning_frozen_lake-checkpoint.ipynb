{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning with FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/ioarun/openai-gym/blob/master/frozenlake/frozenlake-qlearning.py \n",
    "\n",
    "Environment: https://gym.openai.com/ \n",
    "\n",
    "Details: https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note. You need to install gym! Sometimes difficult on Windows. Google for advise.\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\\n\\nA frozenlake-v0 is a 4x4 grid world that looks as follows:\\nSFFF       \\nFHFH       \\nFFFH       \\nHFFG       \\n\\nMeaning of the letters:\\nS: starting point, safe\\nF: frozen surface, safe\\nH: hole, fall to your doom\\nG: goal, where the frisbee is located\\n\\nThe 16 states (position of the agent): \\nState 0: upper left corner (Start)\\n...\\nState 15: Lower right corner (Goal)\\n\\nThe 4 actions (moves of the agent):\\nLEFT = 0,\\nDOWN = 1,\\nRIGHT = 2,\\nUP = 3.\\n\\nReward:\\nThe episode ends when you reach the goal or fall into the water. \\nYou receive a reward of 1 if you reach the goal, and 0 otherwise.\\n\\nEffect of actions:\\n        def inc(row, col, a):\\n            if a == LEFT:\\n                col = max(col-1,0)\\n            elif a == DOWN:\\n                row = min(row+1,nrow-1)\\n            elif a == RIGHT:\\n                col = min(col+1,ncol-1)\\n            elif a == UP:\\n                row = max(row-1,0)\\n            return (row, col)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "A frozenlake-v0 is a 4x4 grid world that looks as follows:\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG       \n",
    "\n",
    "Meaning of the letters:\n",
    "S: starting point, safe\n",
    "F: frozen surface, safe\n",
    "H: hole, fall to your doom\n",
    "G: goal, where the frisbee is located\n",
    "\n",
    "The 16 states (position of the agent): \n",
    "State 0: upper left corner (Start)\n",
    "...\n",
    "State 15: Lower right corner (Goal)\n",
    "\n",
    "The 4 actions (moves of the agent):\n",
    "LEFT = 0,\n",
    "DOWN = 1,\n",
    "RIGHT = 2,\n",
    "UP = 3.\n",
    "\n",
    "Reward:\n",
    "The episode ends when you reach the goal or fall into the water. \n",
    "You receive a reward of 1 if you reach the goal, and 0 otherwise.\n",
    "\n",
    "Effect of actions:\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col-1,0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a == UP:\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecatedEnv",
     "evalue": "Env FrozenLake-v0 not found (valid versions include ['FrozenLake-v1'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'FrozenLake-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74ad6db9136f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FrozenLake-v0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making new env: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    183\u001b[0m             ]\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 raise error.DeprecatedEnv(\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \"Env {} not found (valid versions include {})\".format(\n\u001b[1;32m    187\u001b[0m                         \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Env FrozenLake-v0 not found (valid versions include ['FrozenLake-v1'])"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample actions for exploration:\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 15000 #20000 #60000\n",
    "gamma = 0.95 #0.99\n",
    "learning_rate = 0.7 #0.95 #0.85\n",
    "epsilon = 0.5#1 #0.15 #0.1\n",
    "\n",
    "# initialize the Q table\n",
    "Q = np.zeros([16, 4])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\twhile done == False:\n",
    "        # First we select an action:\n",
    "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
    "\t\t\taction = env.action_space.sample() # Explore action space\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
    "        # Then we perform the action and receive the feedback from the environment\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
    "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "\t\tQ[state,action] += learning_rate*update \n",
    "\t\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let us sanity check some of the Q-values. \n",
    "First we recall what the environment looks like:\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG       \n",
    "\n",
    "And what the 4 actions are:\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q[0])\n",
    "#Should be 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q[3])\n",
    "#Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q[10])\n",
    "#Should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q[14])\n",
    "#Should be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is our Q good enough to guide us from start to goal without falling into the water?\n",
    "state = env.reset()\n",
    "\n",
    "for step in range(10):\n",
    "    env.render()\n",
    "    # Take the action (index) with the maximum expected discounted future reward given that state\n",
    "    action = np.argmax(Q[state,:])\n",
    "    state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
