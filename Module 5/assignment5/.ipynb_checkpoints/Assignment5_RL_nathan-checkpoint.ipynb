{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxVMZneWkPkW"
   },
   "source": [
    "**DAT405 Introduction to Data Science and AI, 2021, Study Period 2** <br/>\n",
    "**Assignment 5: Reinforcement learning and Classification** <br/>\n",
    "**Due Date: Dec 7, 23:59** <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtdoubEvjGwv"
   },
   "source": [
    "**Names and hours spent:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O73TxqjJH7e7"
   },
   "source": [
    "HAUDOT Nathan (20 hours), MATH Hugo (20 hours)\n",
    "\n",
    "---\n",
    "This assignment is about sequential decision making under uncertainty (Reinforcement learning). In a sequential decision process, in each state the decision maker, or agent, chooses among a set of actions, and the system (environment) jumps to a new state based on both the current state and the chosen action. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal policy) that maximizes the rewards. \n",
    "\n",
    "We will use Markov decision processes (MDPs) to model the environment, and the assignment is divided into two parts: \n",
    "-\tFirst, we focus on a decision process with no uncertainty, meaning that we can compute the optimal action in each state. We will use an MDP to model the environment, and then introduce one algorithm (out of many) for finding the optimal policy. \n",
    "- Next, we will use Q-learning to make decisions under uncertainty. Q-learning is a reinforcement learning method that can be used to explore and learn about an unknown environment. The objective is again to determine an optimal policy for the now unknown MDP.\n",
    "\n",
    "We have attached an example notebook illustrating the use of the OpenAI gym library used in questions 3 to 5 (q_learning_frozen_lake.ipynb).\n",
    "\n",
    "##What to Submit\n",
    "\n",
    "**The entire assignment should be submitted through the notebook. No separate file will be accepted.** \n",
    "You have the following options to submit the assignment:\n",
    "-\tSubmit a link to a completed and fully executed Google Colab notebook (please make sure it is executable and editable for anybody with the link).\n",
    "-\tSubmit a completed and fully executed Jupyter notebook (.ipynb-file) from Colab or in Jupyter.\n",
    "\n",
    "\n",
    "**The notebooks should be executed and all code output should be visible and readable.** In Google Colab, first go to the Runtime menu and select Factory Reset-Runtime and then go to the Runtime again and select Run all.\n",
    "\n",
    "\n",
    "*In the notebook:*\n",
    "*\tState your names and how many hours each person spent on the assignment.\n",
    "*\tThe solutions and answers to the theoretical and practical problems, including LaTeX math-mode equations, plots and tables etc.\n",
    "*\tAll plots/results should be visible such that the notebook does not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.<br/>\n",
    "\n",
    "*Before submitting:*\n",
    "*   Make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDuY3qwbH7e7"
   },
   "source": [
    "**Self-check**<br/>\n",
    "Is all the required information included? Have you answered all questions to the best of your ability? Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?) Does your notebook run and can reproduce the results, plots and tables?\n",
    "\n",
    "**Grading**<br/>\n",
    "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
    "*\tPresent clear arguments\n",
    "*\tPresent the results in a pedagogical way\n",
    "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
    "*\tGive correct solutions\n",
    "*\tMake sure that the code is well commented \n",
    "\n",
    "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (Assignment5_RL.ipynb) should be submitted with answers and code written in it. No separate file will be accepted.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_6obY12H7e7"
   },
   "source": [
    "# Primer\n",
    "\n",
    "## Decision Making\n",
    "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
    "two parts. First, how do we learn about the world? This involves both the\n",
    "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
    "currently know about the world, how should we decide what to do, taking into\n",
    "account future events and observations that may change our conclusions?\n",
    "Typically, this will involve creating long-term plans covering possible future\n",
    "eventualities. That is, when planning under uncertainty, we also need to take\n",
    "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
    "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
    "known to produce good results and experiment with something new is known\n",
    "as the **exploration-exploitation dilemma**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC-SVg0ZH7e7"
   },
   "source": [
    "## Overview\n",
    "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
    "\n",
    "\n",
    "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zZ6HjxpH7e7"
   },
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KurOZxYjH7e7"
   },
   "source": [
    "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n",
    "\n",
    "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
    "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
    "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
    "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n",
    "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n",
    "\n",
    "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n",
    "\n",
    "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n",
    "\n",
    "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n",
    "\n",
    "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n",
    "\n",
    "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
    "\n",
    "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^{\\pi^*}(s)$ for all $s\\in S$. That is\n",
    "\n",
    "$$V^{\\pi^*}(s) \\geq V^\\pi(s), s\\in S$$\n",
    "\n",
    "The problem of finding the optimal policy is a _dynamic programming problem_. It turns out that a solution to the optimal policy problem in this context is the *Bellman equation*. The Bellman equation is given by\n",
    "\n",
    "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
    "\n",
    "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
    "\n",
    "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
    "\n",
    "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iUmTgzwH7e7"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWPya78-H7e7"
   },
   "source": [
    "In this first question we work with the deterministic MDP, no code is necessary in this part.\n",
    "\n",
    "Setup:\n",
    "\n",
    "* The state space is a grid with 3 columns and 4 rows, where each state is identified by its position. That is, each state has the form $s = (x,y)$ for $x\\in\\{0,1,2\\}$ and $y \\in\\{0,1,2,3\\}$.\n",
    "* The agent starts in state **A** $=(0,0)$\n",
    "* The actions possible are to move **N** (north), **S** (south), **E** (east), and **W** west.\n",
    "* Note, that you cannot move outside the grid, thus all actions are not available in every box.\n",
    "* When reaching **B** = $(2,3)$, the game ends (absorbing state).\n",
    "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
    "* Assume no discount in this model: $\\gamma = 1$\n",
    "\n",
    "The reward of a state $r(s=(x, y))$ is given by the values on the grid:\n",
    "    \n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|-1 |1|**B**|\n",
    "|0|-1|1|  \n",
    "|-1 |0|-1|  \n",
    "|**A**|-1|1|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YiyQ4Fhpj1L"
   },
   "source": [
    "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjja2rsfqKYi"
   },
   "source": [
    "The optimal path is EENNN, but it is not unique. We can also take the path EENNWNE which takes 2 more moves but for the same reward (which is 0, given that gamma is equal to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkLnxxebppgj"
   },
   "source": [
    "**1b)** What is the optimal policy (i.e. the optimal action in each state)? Is it unique?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAwI0hTvqP-x"
   },
   "source": [
    "An optimal policy lists the best actions to take at each state S for the best reward over time. There can be more than one optimal policy in a MDP : all optimal policy achieve the same optimal value function. Here is the optimal policy of the MDP above :\n",
    "\n",
    "(0,0) : N/E is -1\n",
    "(0,1) : E is 1\n",
    "(0,2) : N/W is -1\n",
    "(1,0) : N/E is 0\n",
    "(1,1) : N/W/S/E is -1\n",
    "(1,2) : N/S is 1\n",
    "(2,0) : N/S/E is -1\n",
    "(2,1) : N/E is 1\n",
    "(2,2) : N is 0\n",
    "(3,0) : E is 1\n",
    "(3,1) : E is 0\n",
    "(3,2) : absorbing state (no possible actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdYsrsFRpsQ4"
   },
   "source": [
    "**1c)** What is expected total reward for the policy in 1b)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Upjs_PbwqRnF"
   },
   "source": [
    "The expected total reward fot the policy defined in 1b is 0. Some states have several possible actions, so we follow all alternative paths until we reach the absorbing state, then we can compare the reward values to find the optimal path. Using the optimal path that we found in 1a, we can compute the reward : -1 + 1 - 1 +1 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyQ7IatcH7e7"
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NfqElM_H7e7"
   },
   "source": [
    "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V$ in the right-hand side (RHS) of the Bellman equation should result in the same $V$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V^*$ which satisfies the Bellman equation. Hence repeated application of the RHS of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what action $a$ achieves the maximum on the RHS of the Bellman equation applied to $V^*$. The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qbn4HjqR2fA"
   },
   "source": [
    "The value iteration algorithm practically procedes as follows:\n",
    "\n",
    "```\n",
    "epsilon is a small value, threshold\n",
    "for x from i to infinity \n",
    "do\n",
    "    for each state s\n",
    "    do\n",
    "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
    "    end\n",
    "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n",
    "        for each state s,\n",
    "        do\n",
    "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
    "            return π, V_k \n",
    "        end\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7hOzat7H7e8"
   },
   "source": [
    "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards $r(s=(x,y))$ given in each state:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|10|0|  \n",
    "|0|0|0| \n",
    "\n",
    "Assume now that given a state $s$ and action $a$, there is a probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken. For instance, if we take action **E** in state $s=(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state, that is, we stay on the grid), and remain still 20 percent of the time. We will use a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
    "\n",
    "Thus, writing the initial value function $V^0(s)$ in each state $s=(x,y)$ in the grid gives:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|0|0|  \n",
    "|0|0|0| \n",
    "\n",
    "\n",
    "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0(s')$ was zero for all $s'$. \n",
    "\n",
    "The updated values $V^1(s)$ for each state become:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|8|0|\n",
    "|8|2|8|  \n",
    "|0|8|0|  \n",
    "  \n",
    "**Iteration 2**:  \n",
    "  \n",
    "Starting with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
    "Action **S**: 0  \n",
    "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **W**: 0\n",
    "\n",
    "Hence any action between **E** and **N** would be best at this stage.\n",
    "\n",
    "Similarly for cell (1,0):\n",
    "\n",
    "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
    "\n",
    "Similar calculations for remaining cells give us the updated value function $V^2(s)$:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|5.76|10.88|5.76|\n",
    "|10.88|8.12|10.88|  \n",
    "|5.76|10.88|5.76|  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccoMLc71H7e8"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "**2a)** Implement the value iteration algorithm just described here in python, and show the converging optimal value function and the optimal policy for the above 3x3 grid. Hint: use the pseudo-code above as a starting point, but be sure to explain what every line does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "627lShCvSABs",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging optimal value = 104\n",
      "\n",
      "Optimal policy values =\n",
      "\n",
      "[[45.61205232 51.9471806  45.61205232]\n",
      " [51.9471806  48.05107671 51.9471806 ]\n",
      " [45.61205232 51.9471806  45.61205232]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "iterations = 0 # Number of iterations the algorithm should make\n",
    "gamma = 0.9 # Reward coefficient\n",
    "actionTaken = 0.8 # Probability that an action is taken\n",
    "epsilon = 0.0001 # Convergence threshold\n",
    "\n",
    "# Original matrices\n",
    "rewards = np.array([[0,0,0],[0,10,0],[0,0,0]]) # Rewards\n",
    "states = np.array([[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]) # Environment\n",
    "new_states = np.array([[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]) # New environment (for computing purpose)\n",
    "\n",
    "# Computing function\n",
    "def computeValue(currentReward, nextReward, currentStateValue, nextStateValue, gammaValue, actionTaken):\n",
    "    v = actionTaken*(nextReward+gammaValue*nextStateValue)+(1-actionTaken)*(currentReward+gammaValue*currentStateValue)\n",
    "    return v\n",
    "\n",
    "# Updating iterations\n",
    "while True:\n",
    "\n",
    "    # States values update\n",
    "    # i = current row\n",
    "    # j = current column\n",
    "    # north, east, west, south = computed V values for each move\n",
    "    \n",
    "    for i in range(0,states.shape[0]):\n",
    "        \n",
    "        for j in range(states.shape[1]):\n",
    "            \n",
    "            # Values reset\n",
    "            north, east, west, south = 0.0,0.0,0.0,0.0\n",
    "            \n",
    "            # N border check\n",
    "            if i != 0:\n",
    "                # Computes north value\n",
    "                north = computeValue(rewards[i,j], rewards[i-1,j], states[i,j], states[i-1,j], gamma, actionTaken)\n",
    "\n",
    "            #E border check\n",
    "            if j != (rewards.shape[0] - 1):\n",
    "                # Computes east value\n",
    "                east = computeValue(rewards[i,j], rewards[i,j+1], states[i,j], states[i,j+1], gamma, actionTaken)\n",
    "            \n",
    "            #W border check\n",
    "            if j != 0:\n",
    "                # Computes west value\n",
    "                west = computeValue(rewards[i,j], rewards[i,j-1], states[i,j], states[i,j-1], gamma, actionTaken)\n",
    "                \n",
    "            #S border check\n",
    "            if i != (rewards.shape[1] - 1):\n",
    "                # Computes south value\n",
    "                south = computeValue(rewards[i,j], rewards[i+1,j], states[i,j], states[i+1,j], gamma, actionTaken)\n",
    "    \n",
    "            # Returns max value in the new states array\n",
    "            new_states[i,j] = max(north, east, south, west)\n",
    "    \n",
    "    #Calculates absolute difference between computed V and previous V values\n",
    "    threshold_state = np.absolute(states - new_states)\n",
    "    \n",
    "    # Threshold condition to check \n",
    "    is_under_threshold = np.all((threshold_state < epsilon))\n",
    "    \n",
    "    # Copy new states array into states array to perform other states values update (or to print final output)\n",
    "    np.copyto(states, new_states)\n",
    "    iterations+=1 # Adds 1 to iteration counter\n",
    "    \n",
    "    # Threshold condition\n",
    "    if is_under_threshold:\n",
    "        print('Converging optimal value = '+str(iterations)+'\\n')\n",
    "        break\n",
    "\n",
    "print('Optimal policy values =\\n')\n",
    "print(states) # Shows final states matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define our optimal policy such as (each case indicates what case \"i,j\" to go next) :\n",
    "\n",
    "[[(0,1) (1,1) (0,1)]\n",
    "\n",
    "[(1,1) (0,1) (1,1)]\n",
    "\n",
    "[(1,0) (1,1) (1,2)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9j_67p0SEH5"
   },
   "source": [
    "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlxjxLdlSFl0"
   },
   "source": [
    "At each iteration, the values of the rewards progressively decrease (gamma factor). Regardless of the initial values chosen for V, the optimal policy found by our algorithm will always converge to the same one. If the initial values are large, the algorithm will have to perform more iterations to achieve convergence (convergence is finite when the difference between the values of the matrices Vt and Vt-1 are less than epsilon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQXoOa7LH7e8"
   },
   "source": [
    "## Reinforcement Learning (RL)\n",
    "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(a,s,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
    "\n",
    "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "The value function and the action-value function are directly related through\n",
    "\n",
    "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
    "\n",
    "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s\n",
    "]\\,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
    "\n",
    "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
    "\n",
    "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n",
    "\n",
    "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there is sufficient exploration. While a constant $\\alpha$ generally does not guarantee us to reach true convergence, we keep it constant at $\\alpha=0.1$ for this assignment.\n",
    "\n",
    "## OpenAI Gym\n",
    "\n",
    "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n",
    "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
    "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jqi0Uqx_TXMp"
   },
   "source": [
    "## Question 3\n",
    "**3a)** You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Convergence is **not** a constant value, rather a stable plateau with some noise. Take $\\gamma=0.95$. You can refer to the Q-learning (frozen lake) Jupyter notebook shown in class, uploaded on Canvas. Hint: start with a small learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61.55435724, 60.89315909],\n",
       "       [65.68790818, 62.05352688],\n",
       "       [70.53839146, 62.97997254],\n",
       "       [73.51755711, 61.54832344],\n",
       "       [87.32252458, 70.85334511]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import gym_toytext\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Setting up environment\n",
    "env = gym.make(\"NChain-v0\") # Imports environment\n",
    "env.reset() # Sets environment back to initial conditions\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 2000 # Number of episodes\n",
    "learning_rate = 0.25 # Learning rate of the alg.\n",
    "gamma = 0.95 # Discount factor\n",
    "epsilon = 0.5 # \"Take a random action\" rate\n",
    "\n",
    "# Setting up Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Training the Q-table\n",
    "for _ in range(episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\twhile done == False:\n",
    "        \n",
    "        # Action selection\n",
    "\t\tif random.uniform(0, 1) < epsilon: # Random under epsilon\n",
    "\t\t\taction = env.action_space.sample() # Explore action space (taking a random action)\n",
    "            \n",
    "\t\telse: # Exploit what we know\n",
    "\t\t\taction = np.argmax(Q[state,:]) # Use learned values\n",
    "\n",
    "        # Performing action & receiving feedback from env.\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Learning by updating Q-value of the selected action\n",
    "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "\t\tQ[state,action] += learning_rate*update \n",
    "\t\tstate = new_state\n",
    "\n",
    "env.close() # Closing the environment\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HlNXl_xTof0"
   },
   "source": [
    "**3b)** Does the result seem reasonable? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yHfjPiUUY6V"
   },
   "source": [
    "The result seems reasonable. It can surely be improved by further tweaking the training parameters. What is positive is that we can see in our Q-table that at state n.5, the algorithm \"understood\" that taking action \"a\" was more beneficial for it than action \"b\" (this is the most significant and visible case). In general, the algorithm has understood that action \"a\" (which does not offer instant rewards) is more beneficial than action \"b\". We can almost understand that the algorithm makes an effort, after understanding the environment, not to take the fastest rewards to get the biggest reward at state n.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enQ-qfVATbue"
   },
   "source": [
    "## Question 4\n",
    "\n",
    "**4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. Hint: compare values obtained using value iteration and Q-learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iPmeeU8VjAq"
   },
   "source": [
    "The Markov Decision Process of the \"chain\" problem has a difference compared to the first case studied with a 3x3 grid: the rewards are related to the actions (transitions between states) more than to the states of the environment. We will not use a reward matrix, but we will choose directly to give the reward corresponding to the action on a modified version of the \"computeValue\" function of the question 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61.3794816 64.8912896 69.5120896 75.5920896 83.5920896]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 2000 # Number of episodes\n",
    "gamma = 0.95 # Discount factor\n",
    "actionTaken = 0.8 # Probability that an action is taken\n",
    "\n",
    "# Modified version of the computeValue function\n",
    "def computeValue(stateValue, statesArray, gammaValue, actionTaken):\n",
    "    if (stateValue == 4): # If state is n.5 (i = 4)\n",
    "        \n",
    "        actionA = actionTaken*(10+gammaValue*statesArray[stateValue])+(1-actionTaken)*(2+gammaValue*statesArray[0])\n",
    "        actionB = actionTaken*(2+gammaValue*statesArray[0])+(1-actionTaken)*(10+gammaValue*statesArray[stateValue])\n",
    "        \n",
    "    else: # If state is other than n.5\n",
    "        actionA = actionTaken*(0+gammaValue*statesArray[stateValue+1])+(1-actionTaken)*(2+gammaValue*statesArray[0])\n",
    "        actionB = actionTaken*(2+gammaValue*statesArray[0])+(1-actionTaken)*(0+gammaValue*statesArray[stateValue+1])\n",
    "\n",
    "    # Returns max value in the states array\n",
    "    return max(actionA, actionB)\n",
    "\n",
    "# States array\n",
    "states = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "\n",
    "# Updating iterations\n",
    "for e in range(episodes):\n",
    "    \n",
    "    # For each state\n",
    "    for i in range(0,5):\n",
    "        states[i] = computeValue(i, states, gamma, actionTaken) # Updating each state\n",
    "        \n",
    "print(states.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the \"Q-table\" obtained in question 3 with the table of the value iteration algorithm, we find very similar values. The relationship presented earlier in the assignment is almost respected:\n",
    "\n",
    "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
    "\n",
    "The small difference between the state values in each method can potentially be explained by the learning rate of the Q-learning algorithm, or the exploration rate. The iteration rate also depends on the results found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH4nvsUPVfjt"
   },
   "source": [
    "**4b)** What is the importance of exploration in RL? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_t0GrBdVk6o"
   },
   "source": [
    "Exploration in reinforcement learning is of key importance, as it is through it that our agent will test other ways of acting in its environment. Exploiting without exploring is like having a greedy algorithm, where our agent would only make decisions based on the highest expected value.\n",
    "\n",
    "We can take the example of an agent having to choose a recipe. For simplicity, we will call him Mr. Cook. With a greedy algorithm, Mr. Cook would always take the recipe that has been most successful in the past. However, there may be other recipes that will give him greater satisfaction. If he stays with this \"greedy policy\", he will not test new recipes, and will therefore potentially miss other more interesting ones. \n",
    "\n",
    "More generally, with a \"greedy policy\", the RL model may not explore all of its environment and thus may not be aware of all the results of all possible actions and rewards. We must then use an \"epsilon-greedy\" policy to implement the concept of exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWUqaN60H7e8"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "**5a)** Explain how a decision tree works and how it extends to random forests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN8sqLlBYHZV"
   },
   "source": [
    "Decision Trees are a classification method in which, at each step of the algorithm, we split into 2 or more branches features of our data (for example with Gini Impurity measurement, there is also entropy measurement).\n",
    "\n",
    "Random forests is a combinaison of multiple decision trees. This machine learning algorithm is used to make decision on autonomous system for example. What differs from decision trees is that random forests take the output of multiple different decision trees and then average it to get the final decision. This helps us get better overall decision by minimising the variance of the prediction. More explicitly about this method, the nodes represents questions, leaves represent labels and branches represent the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21P0SGVuVq3I"
   },
   "source": [
    "**5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WqSxcvsYJBP"
   },
   "source": [
    "Reinforcement learning differs from supervised learning since there are no labels and the data is provided by exploring the environment. It is a mix between unsupervised learning and supervised learning. Whereas with classification and regression we already have obversations about our environment. We can also say that reinforcement learning algorithms use less tweaking for the parameters since it learns by itself using the environment and the possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wev-_UhcH7e8"
   },
   "source": [
    "\n",
    "# References\n",
    "Primer/text based on the following references:\n",
    "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
    "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment5_RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
